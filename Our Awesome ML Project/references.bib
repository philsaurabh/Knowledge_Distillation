@misc{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{alphafold,
	title={Google Deepmind Alphafold: using AI for scientific discovery 2020},
	 howpublished = {\url{https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery}},
	 year={2020}
}

@article{Kamilaris_2018,
	title={Deep learning in agriculture: A survey},
	volume={147},
	ISSN={0168-1699},
	url={http://dx.doi.org/10.1016/j.compag.2018.02.016},
	DOI={10.1016/j.compag.2018.02.016},
	journal={Computers and Electronics in Agriculture},
	publisher={Elsevier BV},
	author={Kamilaris, Andreas and Prenafeta-Boldú, Francesc X.},
	year={2018},
	month={Apr},
	pages={70–90}
}


@misc{abadi2016tensorflow,
	title={TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}, 
	author={Martín Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Greg S. Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal Jozefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dan Mane and Rajat Monga and Sherry Moore and Derek Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda Viegas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
	year={2016},
	eprint={1603.04467},
	archivePrefix={arXiv},
	primaryClass={cs.DC}
}

@inproceedings{NIPS2017_e1e32e23,
	author = {Chen, Guobin and Choi, Wongun and Yu, Xiang and Han, Tony and Chandraker, Manmohan},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {742--751},
	publisher = {Curran Associates, Inc.},
	title = {Learning Efficient Object Detection Models with Knowledge Distillation},
	url = {https://proceedings.neurips.cc/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf},
	volume = {30},
	year = {2017}
}

@article{lecun2010mnist,
	title={MNIST handwritten digit database},
	author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
	journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
	volume={2},
	year={2010}
}

@TECHREPORT{Krizhevsky09learningmultiple,
	author = {Alex Krizhevsky},
	title = {Learning multiple layers of features from tiny images},
	institution = {},
	year = {2009}
}
@misc{teacherfree,
      title={Revisit Knowledge Distillation: a Teacher-free Framework}, 
      author={Li Yuan and Francis E.H.Tay and Guilin Li and Tao Wang and Jiashi Feng},
      year={2020},
      eprint={1909.11723v2},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{attention,
      title={Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer}, 
      author={Sergey Zagoruyko and Nikos Komodakis},
      year={2017},
      eprint={1612.03928v3},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{fitnets,
      title={FitNets: Hints for Thin Deep Nets}, 
      author={Adriana Romero and Nicolas Ballas and Samira Ebrahimi Kahou and Antoine Chassang and Carlo Gatta and Yoshua Bengio},
      year={2015},
      eprint={arXiv:1412.6550v4},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{assistant,
      title={Improved Knowledge Distillation via Teacher Assistant}, 
      author={Seyed-Iman Mirzadeh and Mehrdad Farajtabar and Ang Li and Nir Levine and Akihiro Matsukawa and Hassan Ghasemzadeh},
      year={2019},
      eprint={arXiv:1902.03393v2},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{chen2017learning,
  title={Learning efficient object detection models with knowledge distillation},
  author={Chen, Guobin and Choi, Wongun and Yu, Xiang and Han, Tony and Chandraker, Manmohan},
  journal={Advances in neural information processing systems},
  volume={30},
  pages={742--751},
  year={2017}
}
@inproceedings{yim2017gift,
  title={A gift from knowledge distillation: Fast optimization, network minimization and transfer learning},
  author={Yim, Junho and Joo, Donggyu and Bae, Jihoon and Kim, Junmo},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4133--4141},
  year={2017}
}
@inproceedings{10.1145/1150402.1150464,
author = {Buciluundefined, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
title = {Model Compression},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150464},
doi = {10.1145/1150402.1150464},
abstract = {Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. PDAs), and where computational power is limited (e.g. hea-ring aids). We present a method for "compressing" large, complex ensembles into smaller, faster models, usually without significant loss in performance.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {535–541},
numpages = {7},
keywords = {model compression, supervised learning},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}
@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}
